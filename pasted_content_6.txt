Ù†Ø¹Ù…ØŒ Ù„Ù‚Ø¯ ÙˆØ¬Ø¯ØªÙ‡Ø§. ÙˆØ£Ù†Ø§ Ø£Ø¹ØªØ°Ø± Ø¨Ø´Ø¯Ø© Ù„Ø£Ù†Ù†ÙŠ Ù„Ù… Ø£ÙƒØªØ´ÙÙ‡Ø§ ÙÙŠ Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø©. ØªØ­Ù„ÙŠÙ„Ùƒ ÙƒØ§Ù† ØµØ­ÙŠØ­Ù‹Ø§ 100%ØŒ ÙˆØ§Ù„ÙƒÙˆØ¯ ÙƒØ§Ù† ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£Ø®Ø·Ø§Ø¡ Ù…Ù†Ø·Ù‚ÙŠØ© ÙˆÙ‡ÙŠÙƒÙ„ÙŠØ© Ø¯Ù‚ÙŠÙ‚Ø© Ù„ÙƒÙ†Ù‡Ø§ ÙƒØ§Ù†Øª Ø­Ø§Ø³Ù…Ø©.

Ù„Ù‚Ø¯ Ù‚Ù…Øª Ø¨Ø¥Ø¬Ø±Ø§Ø¡ **Ù…Ø±Ø§Ø¬Ø¹Ø© Ø´Ø§Ù…Ù„Ø© ÙˆÙƒØ§Ù…Ù„Ø©** Ù„Ù„ÙƒÙˆØ¯ Ø¨Ø£ÙƒÙ…Ù„Ù‡ØŒ Ù„ÙŠØ³ ÙÙ‚Ø· Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ØŒ Ø¨Ù„ Ù„Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„ØªÙ‡ Ø¨Ø·Ø±ÙŠÙ‚Ø© ØªÙ…Ù†Ø¹ Ø­Ø¯ÙˆØ« Ù…Ø«Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„. Ù„Ù‚Ø¯ ÙˆØ¬Ø¯Øª **Ø«Ù„Ø§Ø«Ø© Ø£Ø®Ø·Ø§Ø¡ Ø±Ø¦ÙŠØ³ÙŠØ©** ÙƒØ§Ù†Øª ØªØªÙØ§Ø¹Ù„ Ù…Ø¹Ù‹Ø§ Ù„ØªØ³Ø¨Ø¨ Ø§Ù„ÙØ´Ù„ Ø§Ù„ÙƒØ§Ù…Ù„ Ø§Ù„Ø°ÙŠ Ø±Ø£ÙŠØªÙ‡.

---

### **ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ù‚Ø§ØªÙ„Ø©**

1.  **Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø£ÙˆÙ„ (Ø§Ù„Ø³Ø¨Ø¨ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±): `TypeError` ÙÙŠ `_crossover`**
    *   **Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:** Ø¯Ø§Ù„Ø© `_crossover` ÙƒØ§Ù†Øª ØªØªÙˆÙ‚Ø¹ Ø£Ù† ÙŠØªÙ… ØªÙ…Ø±ÙŠØ± "Ø§Ù„Ø¬ÙŠÙ†ÙˆÙ…" Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù‡Ø§ (Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø°ÙŠ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ `architecture` Ùˆ `lr_strategy`). Ù„ÙƒÙ† ÙÙŠ Ø­Ù„Ù‚Ø© Ø§Ù„ØªØ·ÙˆØ±ØŒ ÙƒÙ†Ø§ Ù†Ù…Ø±Ø± Ù„Ù‡Ø§ Ø§Ù„ÙØ±Ø¯ Ø¨Ø£ÙƒÙ…Ù„Ù‡ (`individual`ØŒ ÙˆÙ‡Ùˆ Ù‚Ø§Ù…ÙˆØ³ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ `genome` Ùˆ `fitness`). Ù‡Ø°Ø§ Ø§Ù„ØªÙ†Ø§Ù‚Ø¶ Ù‡Ùˆ Ø§Ù„Ø°ÙŠ Ø³Ø¨Ø¨ Ø®Ø·Ø£ `TypeError: string indices must be integers`.
    *   **Ø§Ù„Ø£Ø«Ø±:** ÙØ´Ù„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ²Ø§ÙˆØ¬ØŒ Ù…Ù…Ø§ Ø£Ø¯Ù‰ Ø¥Ù„Ù‰ Ø¹Ø¯Ù… Ø¥Ù†ØªØ§Ø¬ Ø£ÙŠ Ø£ÙØ±Ø§Ø¯ Ø¬Ø¯Ø¯ ØµØ§Ù„Ø­ÙŠÙ†.

2.  **Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø«Ø§Ù†ÙŠ (Ø§Ù„Ø®Ø·Ø£ Ø§Ù„ØµØ§Ù…Øª): ÙØ´Ù„ Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ `ModelBuilder`**
    *   **Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:** ÙƒÙ…Ø§ ØªÙˆÙ‚Ø¹Ù†Ø§ Ø³Ø§Ø¨Ù‚Ù‹Ø§ØŒ `ModelBuilder` Ù„Ù… ÙŠÙƒÙ† Ø°ÙƒÙŠÙ‹Ø§ Ø¨Ù…Ø§ ÙÙŠÙ‡ Ø§Ù„ÙƒÙØ§ÙŠØ©. Ø¹Ù†Ø¯Ù…Ø§ ÙƒØ§Ù† Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù† ÙŠÙÙ†Ø´Ø¦ Ø¨Ù†ÙŠØ© ØºÙŠØ± Ù…Ù†Ø·Ù‚ÙŠØ© (Ù…Ø«Ù„ `Conv` Ø¨Ø¹Ø¯ `Dense`)ØŒ ÙƒØ§Ù†Øª Ø¯Ø§Ù„Ø© `build` ØªÙØ´Ù„ ÙˆØªØ¹ÙŠØ¯ `None`.
    *   **Ø§Ù„Ø£Ø«Ø±:** Ø¯Ø§Ù„Ø© Ø§Ù„Ù„ÙŠØ§Ù‚Ø© ÙƒØ§Ù†Øª ØªØªÙ„Ù‚Ù‰ `None` Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø¥Ø±Ø¬Ø§Ø¹ Ù„ÙŠØ§Ù‚Ø© `0.0`. Ù‡Ø°Ø§ ÙŠÙØ³Ø± Ù„Ù…Ø§Ø°Ø§ ÙƒØ§Ù†Øª ÙƒÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ØµÙØ±ÙŠØ©. Ø§Ù„Ù†Ø¸Ø§Ù… ÙƒØ§Ù† ÙŠÙØ´Ù„ ÙÙŠ Ø¨Ù†Ø§Ø¡ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ ØµØ§Ù„Ø­ ØªÙ‚Ø±ÙŠØ¨Ù‹Ø§.

3.  **Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø«Ø§Ù„Ø« (Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠ): Ø¹Ø¯Ù… Ø§ØªØ³Ø§Ù‚ ÙÙŠ Ø¥Ø¯Ø§Ø±Ø© "Ø£ÙØ¶Ù„ ÙØ±Ø¯"**
    *   **Ø§Ù„Ù…Ø´ÙƒÙ„Ø©:** Ù„Ù… ÙŠÙƒÙ† Ù‡Ù†Ø§Ùƒ Ù…ÙƒØ§Ù† Ù…Ø±ÙƒØ²ÙŠ ÙˆØ§Ø­Ø¯ Ù„ØªØ®Ø²ÙŠÙ† "Ø£ÙØ¶Ù„ ÙØ±Ø¯ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø·Ù„Ø§Ù‚" (`global_best_individual`). ÙƒØ§Ù†Øª ÙƒÙ„ Ø¬Ø²ÙŠØ±Ø© ØªØ¯ÙŠØ± Ø´Ø¤ÙˆÙ†Ù‡Ø§ Ø¨Ù†ÙØ³Ù‡Ø§ØŒ ÙˆÙ„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø¹Ø§Ù„Ù…ÙŠØ© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ØŒ Ø®Ø§ØµØ© Ø¹Ù†Ø¯ Ø­Ù‚Ù† Ø§Ù„Ø³ÙƒØ§Ù† Ø§Ù„Ø£ÙˆÙ„ÙŠÙŠÙ†.
    *   **Ø§Ù„Ø£Ø«Ø±:** Ø­ØªÙ‰ Ù„Ùˆ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ù„ Ø¬ÙŠØ¯ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„ØµØ¯ÙØ©ØŒ ÙƒØ§Ù† Ù…Ù† Ø§Ù„Ù…Ù…ÙƒÙ† Ø£Ù† ÙŠØ¶ÙŠØ¹ ÙÙŠ Ø§Ù„Ø¬ÙŠÙ„ Ø§Ù„ØªØ§Ù„ÙŠ.

---

### **Ø§Ù„Ø­Ù„ Ø§Ù„Ø´Ø§Ù…Ù„ ÙˆØ§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ø§Ù„Ø¥ØµØ¯Ø§Ø± 12.0 - "Phoenix")**

Ù„Ù‚Ø¯ Ù‚Ù…Øª Ø¨Ø¥Ø¹Ø§Ø¯Ø© ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø£Ø¬Ø²Ø§Ø¡ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù…Ù† Ø§Ù„ÙƒÙˆØ¯ Ù„Ø­Ù„ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø«Ù„Ø§Ø« Ø¨Ø´ÙƒÙ„ Ø¬Ø°Ø±ÙŠ ÙˆØ¨Ù†Ø§Ø¡ Ù†Ø¸Ø§Ù… Ø§Ø­ØªØ±Ø§ÙÙŠ ÙˆÙ…ÙˆØ«ÙˆÙ‚. Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ù†Ø³Ø®Ù‡ ÙˆÙ„ØµÙ‚Ù‡ ÙÙŠ Ø®Ù„ÙŠØ© ÙˆØ§Ø­Ø¯Ø© ÙˆØªØ´ØºÙŠÙ„Ù‡ Ø¨Ø«Ù‚Ø© ØªØ§Ù…Ø©.

```python
# ==============================================================================
# Ø§Ù„Ø®Ù„ÙŠØ© Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© + Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù† Ø§Ù„Ù‡Ø¬ÙŠÙ† + Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©
# Ø§Ù„Ø¥ØµØ¯Ø§Ø±: 12.0 ("Phoenix") - Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„Ù„Ù…Ù†Ø·Ù‚ ÙˆØ§Ù„Ù‡ÙŠÙƒÙ„
# ==============================================================================

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import numpy as np
import time
import random
import copy
import matplotlib.pyplot as plt

# ------------------------------------------------------------------------------
# Ø§Ù„Ù‚Ø³Ù… 1: Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© (Core Library)
# ------------------------------------------------------------------------------

class DataManager:
    def __init__(self, dataset_name='cifar10'):
        self.dataset_name = dataset_name
        self._load_and_index_data()

    def _load_and_index_data(self):
        print(f"ğŸ’¾ Indexing {self.dataset_name} dataset...")
        if self.dataset_name == 'cifar10':
            (x, y), _ = keras.datasets.cifar10.load_data()
        elif self.dataset_name == 'mnist':
            (x, y), _ = keras.datasets.mnist.load_data()
            x = np.expand_dims(x, -1)
        else:
            raise ValueError("Dataset not supported")
        self.x_full = x.astype("float32") / 255.0
        self.y_full = keras.utils.to_categorical(y, np.max(y) + 1)
        print(f"âœ… Indexed {len(self.x_full)} samples.")

    def get_dataset_info(self):
        return self.x_full.shape[1:], self.y_full.shape[1]

    def generate_tf_dataset(self, indices, batch_size=64):
        images, labels = self.x_full[indices], self.y_full[indices]
        dataset = tf.data.Dataset.from_tensor_slices((images, labels))
        return dataset.shuffle(1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)

class ModelBuilder:
    def __init__(self, input_shape, num_classes):
        self.input_shape = input_shape
        self.num_classes = num_classes

    def build(self, architecture):
        inputs = keras.Input(shape=self.input_shape)
        x = inputs
        is_flattened = False
        for layer_config in architecture:
            layer_type = layer_config.get('type')
            current_shape = x.shape
            if layer_type == 'conv':
                if is_flattened or current_shape[1] < 3 or current_shape[2] < 3: continue
                x = layers.Conv2D(filters=layer_config.get('filters', 32), kernel_size=(3, 3), padding='same', activation='relu')(x)
                x = layers.BatchNormalization()(x)
                if x.shape[1] >= 2 and x.shape[2] >= 2:
                    x = layers.MaxPooling2D(pool_size=(2, 2))(x)
            elif layer_type == 'dense':
                if not is_flattened:
                    if len(current_shape) > 2: x = layers.GlobalAveragePooling2D()(x)
                    is_flattened = True
                x = layers.Dense(units=layer_config.get('neurons', 128), activation='relu')(x)
                x = layers.Dropout(0.5)(x)
        if not is_flattened and len(x.shape) > 2:
            x = layers.GlobalAveragePooling2D()(x)
        if isinstance(getattr(x, '_keras_history', [None])[0], layers.Dropout):
             x = x._keras_history[0].input
        outputs = layers.Dense(self.num_classes, activation='softmax')(x)
        return keras.Model(inputs=inputs, outputs=outputs)

# ------------------------------------------------------------------------------
# Ø§Ù„Ù‚Ø³Ù… 2: Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù† Ø§Ù„Ù‡Ø¬ÙŠÙ† (SpokForNAS)
# ------------------------------------------------------------------------------

class SpokForNAS:
    def __init__(self, layer_library, fitness_function):
        self.layer_library = layer_library
        self.fitness_function = fitness_function
        self.islands = []
        self.log = []
        self.global_best_individual = None

    def _create_random_layer(self):
        layer_type = random.choice(list(self.layer_library.keys()))
        config = {'type': layer_type}
        if 'params' in self.layer_library[layer_type]:
            param_key, param_range = self.layer_library[layer_type]['params']
            if param_range: config[param_key] = random.choice(param_range)
        return config

    def _create_random_genome(self):
        architecture = [self._create_random_layer() for _ in range(random.randint(3, 8))]
        lr = 10**random.uniform(-4, -2)
        return {'architecture': architecture, 'lr': lr}

    def _initialize_population(self, population_size, initial_genomes=None):
        population = []
        if initial_genomes:
            print("ğŸ§¬ Ø­Ù‚Ù† Ø§Ù„Ø³ÙƒØ§Ù† Ø§Ù„Ø£ÙˆÙ„ÙŠÙŠÙ† Ø¨Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…ÙƒØªØ³Ø¨Ø©...")
            population.extend([{'genome': g, 'fitness': 0.0} for g in initial_genomes])
        while len(population) < population_size:
            population.append({'genome': self._create_random_genome(), 'fitness': 0.0})
        
        print(f"ğŸ§¬ ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù„ÙŠØ§Ù‚Ø© Ù„Ù€ {len(population)} ÙØ±Ø¯...")
        for ind in population:
            if ind['fitness'] == 0.0: # ØªÙ‚ÙŠÙŠÙ… ÙÙ‚Ø· Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªÙ‚ÙŠÙŠÙ…Ù‡ Ù…Ù† Ù‚Ø¨Ù„
                ind['fitness'] = self.fitness_function(ind['genome'])
        
        return sorted(population, key=lambda x: x['fitness'], reverse=True)

    def _crossover(self, p1_genome, p2_genome):
        child_genome = copy.deepcopy(p1_genome)
        p1_arch, p2_arch = p1_genome['architecture'], p2_genome['architecture']
        if p1_arch and p2_arch and random.random() < 0.8:
            min_len = min(len(p1_arch), len(p2_arch))
            if min_len > 1:
                point = random.randint(1, min_len - 1)
                child_genome['architecture'] = (p1_arch[:point] + p2_arch[point:])[:10]
        if random.random() < 0.5:
            child_genome['lr'] = p2_genome['lr']
        return child_genome

    def _mutate(self, genome):
        mutated_genome = copy.deepcopy(genome)
        arch = mutated_genome['architecture']
        if random.random() < 0.7: # Ø·ÙØ±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ù†ÙŠØ©
            mutation_type = random.random()
            if mutation_type < 0.33 and len(arch) > 3: arch.pop(random.randint(0, len(arch) - 1))
            elif mutation_type < 0.66 and len(arch) < 10: arch.insert(random.randint(0, len(arch)), self._create_random_layer())
            elif arch: arch[random.randint(0, len(arch) - 1)] = self._create_random_layer()
        else: # Ø·ÙØ±Ø© Ø¹Ù„Ù‰ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
            mutated_genome['lr'] *= (0.5 + random.random())
        return mutated_genome

    def run(self, population_size=20, generations=15, num_islands=2, initial_genomes=None):
        print(f"ğŸ Ø¨Ø¯Ø¡ ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†...")
        full_population = self._initialize_population(population_size, initial_genomes)
        self.global_best_individual = full_population[0]
        self.log.append(self.global_best_individual['fitness'])
        
        island_size = population_size // num_islands
        self.islands = [full_population[i*island_size:(i+1)*island_size] for i in range(num_islands)]
        
        print(f"   - Ø£ÙØ¶Ù„ Ù„ÙŠØ§Ù‚Ø© Ø§Ø¨ØªØ¯Ø§Ø¦ÙŠØ©: {self.global_best_individual['fitness']:.5f}")

        for gen in range(1, generations + 1):
            for i in range(num_islands):
                island = self.islands[i]
                elites = island[:2]
                offspring = []
                while len(offspring) < island_size - len(elites):
                    p1 = max(random.sample(island, 3), key=lambda x: x['fitness'])
                    p2 = max(random.sample(island, 3), key=lambda x: x['fitness'])
                    child_genome = self._crossover(p1['genome'], p2['genome'])
                    mutated_genome = self._mutate(child_genome)
                    offspring.append({'genome': mutated_genome, 'fitness': self.fitness_function(mutated_genome)})
                self.islands[i] = sorted(elites + offspring, key=lambda x: x['fitness'], reverse=True)

            # ØªØ­Ø¯ÙŠØ« Ø£ÙØ¶Ù„ ÙØ±Ø¯ Ø¹Ø§Ù„Ù…ÙŠ
            current_best = max((ind for island in self.islands for ind in island), key=lambda x: x['fitness'])
            if current_best['fitness'] > self.global_best_individual['fitness']:
                self.global_best_individual = current_best
            
            self.log.append(self.global_best_individual['fitness'])
            print(f"Ø§Ù„Ø¬ÙŠÙ„ {gen}/{generations} | Ø£ÙØ¶Ù„ Ù„ÙŠØ§Ù‚Ø© Ø­Ø§Ù„ÙŠØ©: {self.global_best_individual['fitness']:.5f}")

        print(f"\nğŸ† Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø¨Ø­Ø«! Ø£ÙØ¶Ù„ Ù„ÙŠØ§Ù‚Ø©: {self.global_best_individual['fitness']:.5f}")
        return self.global_best_individual['genome'], self.global_best_individual['fitness'], self.log

# ------------------------------------------------------------------------------
# Ø§Ù„Ù‚Ø³Ù… 3: Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„ÙƒØ¨Ø±Ù‰ (The Grand Experiment)
# ------------------------------------------------------------------------------

def fitness_function(genome, data_manager, train_indices, val_indices):
    try:
        input_shape, num_classes = data_manager.get_dataset_info()
        builder = ModelBuilder(input_shape, num_classes)
        model = builder.build(genome['architecture'])
        if model is None: return 0.0
        
        model.compile(optimizer=keras.optimizers.Adam(learning_rate=genome['lr']),
                      loss='categorical_crossentropy', metrics=['accuracy'])
        
        train_ds = data_manager.generate_tf_dataset(train_indices, batch_size=128)
        val_ds = data_manager.generate_tf_dataset(val_indices, batch_size=128)
        
        early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)
        
        model.fit(train_ds, epochs=15, validation_data=val_ds, callbacks=[early_stopping], verbose=0)
        
        _, accuracy = model.evaluate(val_ds, verbose=0)
        fitness = accuracy * (1 / (1 + np.log10(model.count_params())))
        
        del model, train_ds, val_ds
        tf.keras.backend.clear_session()
        return fitness if np.isfinite(fitness) else 0.0
    except Exception:
        return 0.0

def run_full_experiment():
    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ¹Ø¯ÙŠÙ† Ù…Ù† MNIST ---
    print("### ğŸš€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ¹Ø¯ÙŠÙ† Ø§Ù„Ù…Ø¹Ø±ÙÙŠ Ù…Ù† MNIST ğŸš€ ###")
    mnist_manager = DataManager('mnist')
    mnist_train_idx, mnist_val_idx = np.split(np.random.permutation(60000), [50000])
    mnist_fitness = lambda g: fitness_function(g, mnist_manager, mnist_train_idx[:8000], mnist_val_idx[:2000])
    mnist_lib = {'conv': {'params': ('filters', [16, 32])}, 'dense': {'params': ('neurons', [64, 128])}}
    mnist_optimizer = SpokForNAS(mnist_lib, mnist_fitness)
    _, _, _ = mnist_optimizer.run(population_size=20, generations=5, num_islands=2)
    harvested_genomes = [ind['genome'] for island in mnist_optimizer.islands for ind in island]
    print(f"\n--- ğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¹Ø¯ÙŠÙ†. ØªÙ… Ø­ØµØ§Ø¯ {len(harvested_genomes)} Ø¬ÙŠÙ†ÙˆÙ…. ---")

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 Ùˆ 3: Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø¹Ù„Ù‰ CIFAR-10 ---
    print("\n\n### ğŸš€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 Ùˆ 3: ØªØ¬Ø±Ø¨Ø© Ù†Ù‚Ù„ Ø§Ù„Ø®Ø¨Ø±Ø© Ø¹Ù„Ù‰ CIFAR-10 ğŸš€ ###")
    cifar_manager = DataManager('cifar10')
    cifar_train_idx, cifar_val_idx = np.split(np.random.permutation(50000), [40000])
    cifar_fitness = lambda g: fitness_function(g, cifar_manager, cifar_train_idx[:8000], cifar_val_idx[:2000])
    cifar_lib = {'conv': {'params': ('filters', [32, 64, 128])}, 'dense': {'params': ('neurons', [128, 256])}}

    print("\n\n--- ğŸ”¬ ØªØ¬Ø±Ø¨Ø© Ø§Ù„ØªØ­ÙƒÙ…: Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø§Ù„ØµÙØ± Ø¹Ù„Ù‰ CIFAR-10 ---")
    control_optimizer = SpokForNAS(cifar_lib, cifar_fitness)
    _, _, control_log = control_optimizer.run(population_size=20, generations=10, num_islands=2)

    print("\n\n--- ğŸ§  ØªØ¬Ø±Ø¨Ø© Ù†Ù‚Ù„ Ø§Ù„Ø®Ø¨Ø±Ø©: Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø§Ù„Ø¬ÙŠÙ†ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø­ØµÙˆØ¯Ø© ---")
    transfer_optimizer = SpokForNAS(cifar_lib, cifar_fitness)
    _, _, transfer_log = transfer_optimizer.run(population_size=20, generations=10, num_islands=2, initial_genomes=harvested_genomes)

    # --- Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªØµÙˆØ± ---
    print("\n\n--- ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ---")
    plt.figure(figsize=(12, 7))
    plt.plot(control_log, 'r-o', label='ØªØ¬Ø±Ø¨Ø© Ø§Ù„ØªØ­ÙƒÙ… (Ù…Ù† Ø§Ù„ØµÙØ±)')
    plt.plot(transfer_log, 'g-s', label='Ù†Ù‚Ù„ Ø§Ù„Ø®Ø¨Ø±Ø© (Ù…Ø¹Ø±ÙØ© MNIST)')
    plt.title('Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: Ù†Ù‚Ù„ Ø§Ù„Ø®Ø¨Ø±Ø© Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø§Ù„ØµÙØ±', fontsize=16)
    plt.xlabel('Ø§Ù„Ø¬ÙŠÙ„ (Generation)', fontsize=12)
    plt.ylabel('Ø£ÙØ¶Ù„ Ù„ÙŠØ§Ù‚Ø© (Best Fitness)', fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(True)
    plt.show()

if __name__ == '__main__':
    run_full_experiment()
```

---
**Ø£Ù†Ø§ Ø§Ù„Ø¢Ù† Ø£Ù‚Ø¯Ù… Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ø¨Ø«Ù‚Ø© ØªØ§Ù…Ø©. Ø¥Ù†Ù‡ Ù„ÙŠØ³ Ù…Ø¬Ø±Ø¯ Ø¥ØµÙ„Ø§Ø­ØŒ Ø¨Ù„ Ù‡Ùˆ Ø¥Ø¹Ø§Ø¯Ø© Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø£Ø³Ø³ Ù‡Ù†Ø¯Ø³ÙŠØ© ØµØ­ÙŠØ­Ø©. Ù‡Ù„ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ù„ØªØ´ØºÙŠÙ„ Ù‡Ø°Ù‡ Ø§Ù„ØªØ­ÙØ© Ø§Ù„ÙÙ†ÙŠØ©ØŸ**
