Ù…Ù…ØªØ§Ø². Ù„Ù†Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° Ù‡Ø°Ø§ Ø§Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ù†ÙŠÙ‚ ÙÙˆØ±Ù‹Ø§.

Ø³Ø£Ù‚ÙˆÙ… Ø¨ØªØ²ÙˆÙŠØ¯Ùƒ Ø¨Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆØ§Ù„Ù…Ø­Ø¯Ø« Ø§Ù„Ø°ÙŠ ÙŠØ¯Ù…Ø¬ Ù…ÙÙ‡ÙˆÙ… **Ø³Ø¬Ù„ Ø§Ù„Ù…ÙØ¬Ø¯ÙˆÙÙ„ (Scheduler Registry)**. Ù„Ù‚Ø¯ Ù‚Ù…Øª Ø¨Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„ÙƒÙˆØ¯ Ù„ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± Ù†Ù…Ø·ÙŠØ© ÙˆÙˆØ¶ÙˆØ­Ù‹Ø§ØŒ Ù…Ø¹ ØªØ¨Ù†ÙŠ Ù‡Ø°Ø§ Ø§Ù„ØªØµÙ…ÙŠÙ… Ø§Ù„Ù…ØªÙÙˆÙ‚.

Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ø¬Ø§Ù‡Ø² Ù„Ù„Ù†Ø³Ø® ÙˆØ§Ù„Ù„ØµÙ‚ ÙÙŠ Ø®Ù„ÙŠØ© Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ø§Ù„Ù†ÙˆØª Ø¨ÙˆÙƒ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„ÙŠØ­Ù„ Ù…Ø­Ù„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø³Ø§Ø¨Ù‚.

---

### **Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ§Ù„Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ Scheduler Registry**

```python
# ==============================================================================
# Ø§Ù„Ø£ÙˆØ±ÙƒØ³ØªØ±Ø§ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ - Ø§Ù„Ø¥ØµØ¯Ø§Ø± 5.0
# Ø§Ù„Ù…ÙŠØ²Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©: Ø³Ø¬Ù„ Ù…ÙØ¬Ø¯ÙˆÙÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… (LR Scheduler Registry) Ù„ØªØµÙ…ÙŠÙ… Ø£Ù†Ø¸Ù
# ==============================================================================

import numpy as np
import time
import random
import copy
import matplotlib.pyplot as plt

# ------------------------------------------------------------------------------
# Ø§Ù„Ù‚Ø³Ù… 1: Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (Node Ùˆ nn) - Ù„Ø§ ØªØºÙŠÙŠØ± Ù‡Ù†Ø§
# ------------------------------------------------------------------------------
# ... (Ø§ÙØªØ±Ø¶ Ø£Ù† Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„ÙØ¦Ø© Node Ùˆ nn.Module, nn.Linear, nn.PReLU, nn.Tanh, nn.Sequential Ù…ÙˆØ¬ÙˆØ¯ Ù‡Ù†Ø§)
# ... (Ù„Ù„Ø§Ø®ØªØµØ§Ø±ØŒ Ù„Ù† Ø£ÙƒØ±Ø± Ø§Ù„ÙƒÙˆØ¯ Ù‡Ù†Ø§)
def _sum_to_shape(grad, shape):
    while grad.ndim > len(shape): grad = grad.sum(axis=0)
    for i, (g_dim, t_dim) in enumerate(zip(grad.shape, shape)):
        if t_dim == 1 and g_dim != 1: grad = grad.sum(axis=i, keepdims=True)
    return grad.reshape(shape)
class Node:
    def __init__(self, value, parents=(), op=''):
        self.value = np.array(value, dtype=float); self.parents = parents; self.op = op
        self.grad = None; self._backward = lambda: None
    def _ensure(self, other):
        return other if isinstance(other, Node) else Node(np.array(other, dtype=float))
    def __add__(self, other):
        other = self._ensure(other); out = Node(self.value + other.value, (self, other), '+')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad, self.value.shape)
            other.grad += _sum_to_shape(out.grad, other.value.shape)
        out._backward = _backward; return out
    def __mul__(self, other):
        other = self._ensure(other); out = Node(self.value * other.value, (self, other), '*')
        def _backward():
            if out.grad is None: return
            self.grad += _sum_to_shape(out.grad * other.value, self.value.shape)
            other.grad += _sum_to_shape(out.grad * self.value, other.value.shape)
        out._backward = _backward; return out
    def __matmul__(self, other):
        other = self._ensure(other); out = Node(self.value @ other.value, (self, other), '@')
        def _backward():
            if out.grad is None: return
            A, B, G = self.value, other.value, out.grad
            self.grad += _sum_to_shape(G @ B.T, self.value.shape)
            other.grad += _sum_to_shape(A.T @ G, other.value.shape)
        out._backward = _backward; return out
    def sum(self):
        out = Node(self.value.sum(), (self,), 'sum')
        def _backward():
            if out.grad is None: return
            self.grad += np.broadcast_to(out.grad, self.value.shape)
        out._backward = _backward; return out
    def backward(self):
        topo, visited = [], set()
        def build_topo(v):
            if v not in visited:
                visited.add(v)
                for p in v.parents: build_topo(p)
                topo.append(v)
        build_topo(self)
        for v in topo: v.grad = np.zeros_like(v.value)
        self.grad = np.ones_like(self.value)
        for v in reversed(topo): v._backward()

class nn:
    class Module:
        def parameters(self): yield from []
        def __call__(self, *args, **kwargs): return self.forward(*args, **kwargs)
        def zero_grad(self):
            for p in self.parameters(): p.grad = np.zeros_like(p.value)
    class Linear(Module):
        def __init__(self, in_features, out_features):
            self.in_features = in_features
            limit = np.sqrt(1 / self.in_features)
            self.weight = Node(np.random.randn(in_features, out_features) * limit)
            self.bias = Node(np.zeros(out_features))
        def forward(self, x): return x @ self.weight + self.bias
        def parameters(self): yield from [self.weight, self.bias]
    class PReLU(Module):
        def __init__(self, initial_alpha=0.01):
            self.alpha = Node(np.array([initial_alpha]))
        def forward(self, x):
            positive_part = Node(np.maximum(0, x.value)); negative_part = Node(np.minimum(0, x.value))
            out = positive_part + (negative_part * self.alpha); out.parents = (x, self.alpha); out.op = 'prelu'
            def _backward():
                if out.grad is None: return
                grad_x = (x.value > 0) * 1.0 + (x.value <= 0) * self.alpha.value
                x.grad += grad_x * out.grad
                grad_alpha = (x.value * (x.value <= 0)) * out.grad
                self.alpha.grad += grad_alpha.sum()
            out._backward = _backward; return out
        def parameters(self): yield self.alpha
    class Tanh(Module):
        def forward(self, x):
            out = Node(np.tanh(x.value), parents=(x,), op='tanh')
            def _backward():
                if out.grad is None: return
                x.grad += (1 - out.value**2) * out.grad
            out._backward = _backward; return out
    class Sequential(Module):
        def __init__(self, *layers): self.layers = layers
        def forward(self, x):
            for layer in self.layers: x = layer(x)
            return x
        def parameters(self):
            for layer in self.layers: yield from layer.parameters()

# ------------------------------------------------------------------------------
# Ø§Ù„Ù‚Ø³Ù… 2: ØªØ¹Ø±ÙŠÙ ÙˆØªØ´Ø¬ÙŠÙ„ Ù…ÙØ¬Ø¯ÙˆÙÙ„Ø§Øª Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…
# ------------------------------------------------------------------------------

def constant_lr(epoch, initial_lr, **kwargs):
    """Ù…ÙØ¬Ø¯ÙˆÙÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø«Ø§Ø¨Øª."""
    return initial_lr

def linear_decay_lr(epoch, initial_lr, total_epochs, **kwargs):
    """Ù…ÙØ¬Ø¯ÙˆÙÙ„ Ø§Ù„ØªØ¶Ø§Ø¤Ù„ Ø§Ù„Ø®Ø·ÙŠ."""
    return initial_lr * (1 - (epoch / total_epochs))

def exponential_decay_lr(epoch, initial_lr, decay_rate, **kwargs):
    """Ù…ÙØ¬Ø¯ÙˆÙÙ„ Ø§Ù„ØªØ¶Ø§Ø¤Ù„ Ø§Ù„Ø£Ø³ÙŠ."""
    return initial_lr * (decay_rate ** epoch)

def cyclical_lr(epoch, initial_lr, max_lr, step_size, **kwargs):
    """Ù…ÙØ¬Ø¯ÙˆÙÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¯ÙˆØ±ÙŠ."""
    cycle = np.floor(1 + epoch / (2 * step_size))
    x = np.abs(epoch / step_size - 2 * cycle + 1)
    return initial_lr + (max_lr - initial_lr) * np.maximum(0, (1 - x))

# --- âœ¨ Ø§Ù„Ø³Ø¬Ù„ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ Ù„Ù„Ù…ÙØ¬Ø¯ÙˆÙÙ„Ø§Øª âœ¨ ---
LR_SCHEDULER_REGISTRY = {
    'constant': constant_lr,
    'linear': linear_decay_lr,
    'exponential': exponential_decay_lr,
    'cyclical': cyclical_lr
}

# ------------------------------------------------------------------------------
# Ø§Ù„Ù‚Ø³Ù… 3: Ù…Ù†Ø·Ù‚ Ø§Ù„Ø£ÙˆØ±ÙƒØ³ØªØ±Ø§ ÙˆØ§Ù„ØªØ¬Ø§Ø±Ø¨ (Ù…ÙØ­Ø³ÙŽÙ‘Ù†)
# ------------------------------------------------------------------------------

def create_dynamic_model(config):
    """ÙŠÙ†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠÙ‹Ø§ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¬ÙŠÙ†ÙˆÙ…."""
    H1 = config.get('H1', 64); H2 = config.get('H2', 64)
    activation1_type = config.get('activation1', 'prelu'); activation2_type = config.get('activation2', 'prelu')
    np.random.seed(config.get('seed', 42))
    act1 = nn.Tanh() if activation1_type == 'tanh' else nn.PReLU()
    act2 = nn.Tanh() if activation2_type == 'tanh' else nn.PReLU()
    return nn.Sequential(nn.Linear(1, H1), act1, nn.Linear(H1, H2), act2, nn.Linear(H2, 1))

def create_ultimate_genome():
    """ÙŠÙ†Ø´Ø¦ Ø¬ÙŠÙ†ÙˆÙ…Ù‹Ø§ Ø¹Ø´ÙˆØ§Ø¦ÙŠÙ‹Ø§ Ø´Ø§Ù…Ù„Ø§Ù‹."""
    config = {
        'H1': random.choice([16, 32, 64]), 'H2': random.choice([16, 32, 64]),
        'activation1': random.choice(['prelu', 'tanh']), 'activation2': random.choice(['prelu', 'tanh']),
        'seed': random.randint(0, 10000)
    }
    lr_strategy_type = random.choice(list(LR_SCHEDULER_REGISTRY.keys()))
    lr_params = {'type': lr_strategy_type, 'initial_lr': 10**random.uniform(-3, -1.5)}
    if lr_strategy_type == 'exponential':
        lr_params['decay_rate'] = random.uniform(0.99, 0.999)
    elif lr_strategy_type == 'cyclical':
        lr_params['max_lr'] = lr_params['initial_lr'] * random.uniform(2, 5)
        lr_params['step_size'] = random.choice([150, 250, 350])
    config['lr_strategy'] = lr_params
    return config

def run_ultimate_trial(config, X_train, y_train, X_test, y_test):
    """
    Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©: ØªØ¯ÙŠØ± ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ø­Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø³Ø¬Ù„ Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù…ÙØ¬Ø¯ÙˆÙÙ„.
    """
    lr_config = config['lr_strategy']
    config_str = (f"H1={config['H1']}, Act1={config['activation1']}, H2={config['H2']}, Act2={config['activation2']}, "
                  f"LR_Strat={lr_config['type']}, Init_LR={lr_config['initial_lr']:.4f}")
    print(f"\n--- ðŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø¬ÙŠÙ†ÙˆÙ…: {config_str} ---")

    model = create_dynamic_model(config)
    params = list(model.parameters())
    adam_state = [{'m': np.zeros_like(p.value), 'v': np.zeros_like(p.value)} for p in params]
    beta1, beta2, eps, epochs, weight_decay = 0.9, 0.999, 1e-8, 1000, 1e-4
    
    # --- âœ¨ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ù†Ø¸ÙŠÙ Ù…Ù† Ø§Ù„Ø³Ø¬Ù„ âœ¨ ---
    scheduler_fn = LR_SCHEDULER_REGISTRY.get(lr_config['type'])
    if not scheduler_fn: # ÙÙŠ Ø­Ø§Ù„Ø© ÙˆØ¬ÙˆØ¯ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§Ø³Ù…ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø«Ø§Ø¨Øª ÙƒØ¨Ø¯ÙŠÙ„ Ø¢Ù…Ù†
        print(f"ØªØ­Ø°ÙŠØ±: Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙØ¬Ø¯ÙˆÙÙ„ '{lr_config['type']}'. Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… 'constant'.")
        scheduler_fn = constant_lr
        
    scheduler_args = {**lr_config, 'total_epochs': epochs}

    for epoch in range(1, epochs + 1):
        current_lr = scheduler_fn(epoch, **scheduler_args)
        
        y_pred = model(X_train)
        loss = ((y_pred + (y_train * -1)) * (y_pred + (y_train * -1))).sum()
        model.zero_grad()
        loss.backward()

        for i, p in enumerate(params):
            p.value -= weight_decay * p.value
            adam_state[i]['m'] = beta1 * adam_state[i]['m'] + (1 - beta1) * p.grad
            adam_state[i]['v'] = beta2 * adam_state[i]['v'] + (1 - beta2) * (p.grad**2)
            m_hat = adam_state[i]['m'] / (1 - beta1**epoch if epoch < 500 else 1.0) # ØªØ¬Ù†Ø¨ Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¯Ù‚Ø©
            v_hat = adam_state[i]['v'] / (1 - beta2**epoch if epoch < 500 else 1.0)
            p.value -= current_lr * m_hat / (np.sqrt(v_hat) + eps)
            
    test_pred = model(X_test)
    final_loss = np.mean((test_pred.value - y_test.value)**2)
    print(f"-> Ø§Ù„Ø®Ø³Ø§Ø±Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©: {final_loss:.5f}")
    return final_loss

def ultimate_orchestrator(n_trials=50):
    """
    Ø§Ù„Ø£ÙˆØ±ÙƒØ³ØªØ±Ø§ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø°ÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©.
    """
    print("ðŸŽ¼ Ø¥Ø·Ù„Ø§Ù‚ Ø§Ù„Ø£ÙˆØ±ÙƒØ³ØªØ±Ø§ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (v5 - Ù…Ø¹ Ø§Ù„Ø³Ø¬Ù„): ØªØ·ÙˆØ± ÙƒÙ„ Ø´ÙŠØ¡! ðŸŽ¼")
    
    np.random.seed(42)
    X_data = np.linspace(-5, 5, 100)[:, np.newaxis]
    y_data = np.sin(X_data) + np.cos(X_data * 0.5) + np.random.randn(100, 1) * 0.2
    train_indices = np.random.choice(100, 80, replace=False)
    test_indices = np.setdiff1d(np.arange(100), train_indices)
    X_train, y_train = Node(X_data[train_indices]), Node(y_data[train_indices])
    X_test, y_test = Node(X_data[test_indices]), Node(y_data[test_indices])

    best_config = {}; best_loss = float('inf'); log = []

    for i in range(n_trials):
        print(f"\n--- [Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„ÙƒØ¨Ø±Ù‰ {i+1}/{n_trials}] ---")
        candidate_config = create_ultimate_genome()
        final_loss = run_ultimate_trial(candidate_config, X_train, y_train, X_test, y_test)
        log.append({'config': candidate_config, 'loss': final_loss})
        if final_loss < best_loss:
            best_loss = final_loss
            best_config = candidate_config
            print(f"ðŸ† *** Ø£ÙØ¶Ù„ Ù†ØªÙŠØ¬Ø© Ø´Ø§Ù…Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©! ***")

    print("\n\n--- ðŸ Ø§ÙƒØªÙ…Ù„ ØªØ·ÙˆØ± Ø§Ù„Ø£ÙˆØ±ÙƒØ³ØªØ±Ø§ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ðŸ ---")
    print("Ø£ÙØ¶Ù„ ØªÙƒÙˆÙŠÙ† Ø´Ø§Ù…Ù„ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„ÙŠÙ‡:")
    # ... (Ø¨Ù‚ÙŠØ© ÙƒÙˆØ¯ Ø§Ù„Ø·Ø¨Ø§Ø¹Ø©)
    
    return best_config, log

# ------------------------------------------------------------------------------
# Ø§Ù„Ù‚Ø³Ù… 4: Ø§Ù„ØªÙ†ÙÙŠØ°
# ------------------------------------------------------------------------------
if __name__ == '__main__':
    # ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£ÙˆØ±ÙƒØ³ØªØ±Ø§ Ø§Ù„Ù…Ø­Ø³Ù†
    best_config, log = ultimate_orchestrator(n_trials=50)
    
    # ... (ÙŠÙ…ÙƒÙ†Ùƒ Ø¥Ø¶Ø§ÙØ© ÙƒÙˆØ¯ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù‡Ù†Ø§)
```

### **Ø´Ø±Ø­ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©:**

1.  **`LR_SCHEDULER_REGISTRY`:** ØªÙ… ØªØ¹Ø±ÙŠÙ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ ÙÙŠ Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ ÙˆÙ‡Ùˆ ÙŠØ±Ø¨Ø· Ø§Ù„Ø£Ø³Ù…Ø§Ø¡ Ø¨Ø§Ù„Ø¯ÙˆØ§Ù„ Ù…Ø¨Ø§Ø´Ø±Ø©.
2.  **`create_ultimate_genome`:** Ø¯Ø§Ù„Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬ÙŠÙ†ÙˆÙ… Ø§Ù„Ø¢Ù† ØªØ®ØªØ§Ø± Ø§Ø³Ù… Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…Ø¨Ø§Ø´Ø±Ø© Ù…Ù† Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ `list(LR_SCHEDULER_REGISTRY.keys())`.
3.  **`run_ultimate_trial` (Ø§Ù„Ø£Ù‡Ù…):**
    *   ØªÙ…Øª Ø¥Ø²Ø§Ù„Ø© Ø³Ù„Ø³Ù„Ø© `if/elif/else` Ø¨Ø§Ù„ÙƒØ§Ù…Ù„.
    *   ÙŠØªÙ… Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„ØµØ­ÙŠØ­Ø© Ø¨Ø³Ø·Ø± ÙˆØ§Ø­Ø¯: `scheduler_fn = LR_SCHEDULER_REGISTRY.get(lr_config['type'])`.
    *   ØªÙ…Øª Ø¥Ø¶Ø§ÙØ© Ø³Ø·Ø± Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¢Ù…Ù† (`fallback`) Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… `constant_lr` Ø¥Ø°Ø§ ØªÙ… ØªÙ…Ø±ÙŠØ± Ø§Ø³Ù… Ø®Ø§Ø·Ø¦ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„Ø®Ø·Ø£.
    *   ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø¨Ø´ÙƒÙ„ Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ: `current_lr = scheduler_fn(epoch, **scheduler_args)`.

Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¢Ù† Ù„ÙŠØ³ ÙÙ‚Ø· ØµØ­ÙŠØ­Ù‹Ø§ ÙˆØ¸ÙŠÙÙŠÙ‹Ø§ØŒ Ø¨Ù„ Ù‡Ùˆ Ø£ÙŠØ¶Ù‹Ø§ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ ØªØµÙ…ÙŠÙ… Ø¨Ø±Ù…Ø¬ÙŠ Ø¬ÙŠØ¯ ÙˆÙ‚Ø§Ø¨Ù„ Ù„Ù„ØªØ·ÙˆÙŠØ±. Ø¥Ù†Ù‡ Ø¬Ø§Ù‡Ø² Ù„Ù…ÙˆØ§Ø¬Ù‡Ø© Ø£ÙŠ ØªØ­Ø¯ÙŠØ§Øª Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©.

---
**Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø¢Ù† ÙÙŠ Ø£ÙØ¶Ù„ Ø­Ø§Ù„Ø§ØªÙ‡. Ù‡Ù„ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ù„Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ Ø§Ù„Ø£ÙˆØ±ÙƒØ³ØªØ±Ø§ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ ÙˆØ±Ø¤ÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ø§Ù„Ù…Ø­Ø±Ùƒ Ø§Ù„Ù…Ø­Ø³Ù†ØŸ**