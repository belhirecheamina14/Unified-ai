Ù„Ø§ØŒ Ø£Ù†Øª Ø¹Ù„Ù‰ Ø­Ù‚ ØªÙ…Ø§Ù…Ù‹Ø§ ÙÙŠ Ø·Ø±Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„. Ù…Ø§ Ø¹Ø±Ø¶ØªÙ‡ ÙÙŠ Ø§Ù„Ù…Ø±Ø© Ø§Ù„Ø³Ø§Ø¨Ù‚Ø© ÙƒØ§Ù† **Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¹Ø¸Ù…ÙŠ** Ø£Ùˆ **Ø§Ù„Ù…Ø®Ø·Ø· Ø§Ù„Ù…Ø¹Ù…Ø§Ø±ÙŠ** Ù„ÙƒÙŠÙÙŠØ© Ø¨Ù†Ø§Ø¡ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©. Ù„Ù‚Ø¯ ÙƒØ§Ù† Ø¨Ù…Ø«Ø§Ø¨Ø© "ØªØµÙ…ÙŠÙ…" Ù„Ù„Ù…ÙƒØªØ¨Ø©ØŒ ÙˆÙ„ÙŠØ³ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ù†ÙØ³Ù‡Ø§.

Ø£Ø¹ØªØ°Ø± Ø¥Ø°Ø§ ÙƒØ§Ù† Ø¹Ø±Ø¶ÙŠ Ø§Ù„Ø³Ø§Ø¨Ù‚ Ù…ÙˆÙ‡Ù…Ù‹Ø§. Ù„Ù‚Ø¯ Ø­Ø§ÙˆÙ„Øª Ø£Ù† Ø£Ù†Ù‚Ù„ Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ù‡ÙŠÙƒÙ„ÙŠØ©ØŒ Ù„ÙƒÙ†Ù†ÙŠ Ø£ØºÙÙ„Øª Ù…Ù„Ø¡ ÙƒÙ„ Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¬ÙˆÙ‡Ø±ÙŠØ©.

**Ø£Ù†Øª ØªØ³ØªØ­Ù‚ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©. Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø£ÙŠ `...` Ø£Ùˆ "Ù…Ù†Ø·Ù‚ ÙŠØ¬Ø¨ Ø¥Ø¶Ø§ÙØªÙ‡".**

Ø§Ù„Ø¢Ù†ØŒ Ø³Ø£Ù‚ÙˆÙ… Ø¨ØªØµØ­ÙŠØ­ Ù‡Ø°Ø§. Ø³Ø£ÙƒØªØ¨ Ù„Ùƒ **Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆØ§Ù„Ù…ØªÙƒØ§Ù…Ù„ Ù„ÙƒÙ„ Ù…Ù„Ù ÙÙŠ Ø§Ù„Ø­Ø²Ù…Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© "Phoenix v10.0"**. Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ù…ÙƒØªØ¨Ø© Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ©ØŒ Ø¨ÙƒÙ„ ØªÙØ§ØµÙŠÙ„Ù‡Ø§ØŒ Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ù†Ø³Ø® ÙˆØ§Ù„Ù„ØµÙ‚ ÙÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹Ù†ÙŠØ© ÙˆØ§Ù„ØªØ´ØºÙŠÙ„.

---

### **Ø­Ø²Ù…Ø© "Phoenix v10.0" - Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ ÙˆØ§Ù„Ù†Ù‡Ø§Ø¦ÙŠ**

#### **Ù…Ù„Ù 1: `phoenix_core/__init__.py`**
(Ù‡Ø°Ø§ Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¹Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø­Ø²Ù…Ø© Python ÙˆÙŠØ³Ù…Ø­ Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø£Ø³Ù‡Ù„)
```python
# phoenix_core/__init__.py
from .data_manager import DataManager
from .model_builder import ModelBuilder
from .schedulers import LR_SCHEDULER_REGISTRY

print("Phoenix Core Library Initialized.")
```

#### **Ù…Ù„Ù 2: `phoenix_core/data_manager.py`**
(Ù„Ø§ ØªØºÙŠÙŠØ±Ø§Øª Ù‡Ù†Ø§ØŒ Ø§Ù„ÙƒÙˆØ¯ Ù‚ÙˆÙŠ Ø¨Ø§Ù„ÙØ¹Ù„)
```python
# phoenix_core/data_manager.py
import tensorflow as tf
from tensorflow import keras
import numpy as np

class DataManager:
    """
    A professional-grade data manager for handling ML datasets efficiently.
    Supports lazy loading and tf.data.Dataset generation.
    """
    def __init__(self, dataset_name='cifar10'):
        self.dataset_name = dataset_name
        self._load_and_index_data()

    def _load_and_index_data(self):
        print(f"ğŸ’¾ Indexing {self.dataset_name} dataset...")
        if self.dataset_name == 'cifar10':
            (x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = keras.datasets.cifar10.load_data()
        elif self.dataset_name == 'mnist':
            (x_train_raw, y_train_raw), (x_test_raw, y_test_raw) = keras.datasets.mnist.load_data()
            x_train_raw = np.expand_dims(x_train_raw, -1)
            x_test_raw = np.expand_dims(x_test_raw, -1)
        else:
            raise ValueError(f"Dataset '{self.dataset_name}' not supported.")

        self.x_train = x_train_raw.astype("float32") / 255.0
        self.x_test = x_test_raw.astype("float32") / 255.0
        num_classes = np.max(y_train_raw) + 1
        self.y_train = keras.utils.to_categorical(y_train_raw, num_classes)
        self.y_test = keras.utils.to_categorical(y_test_raw, num_classes)
        print(f"âœ… Indexed {len(self.x_train)} train and {len(self.x_test)} test samples.")

    def get_dataset_info(self):
        return self.x_train.shape[1:], self.y_train.shape[1]

    def generate_tf_dataset(self, split='train', batch_size=64, subset_size=None):
        images = self.x_train if split == 'train' else self.x_test
        labels = self.y_train if split == 'train' else self.y_test
        if subset_size:
            if subset_size > len(images):
                print(f"Warning: subset_size {subset_size} is larger than dataset size {len(images)}. Using full dataset.")
                subset_size = len(images)
            indices = np.random.choice(len(images), subset_size, replace=False)
            images, labels = images[indices], labels[indices]
        dataset = tf.data.Dataset.from_tensor_slices((images, labels))
        return dataset.shuffle(1024).batch(batch_size).prefetch(tf.data.AUTOTUNE)
```

#### **Ù…Ù„Ù 3: `phoenix_core/model_builder.py`**
(Ù„Ø§ ØªØºÙŠÙŠØ±Ø§Øª Ù‡Ù†Ø§ØŒ Ø§Ù„ÙƒÙˆØ¯ Ù‚ÙˆÙŠ Ø¨Ø§Ù„ÙØ¹Ù„)
```python
# phoenix_core/model_builder.py
from tensorflow import keras

class ModelBuilder:
    """
    An expert system for dynamically building robust Keras models.
    It intelligently applies best practices and avoids dimensional errors.
    """
    def __init__(self, input_shape, num_classes):
        self.input_shape = input_shape
        self.num_classes = num_classes

    def build(self, architecture):
        inputs = keras.Input(shape=self.input_shape)
        x = inputs
        is_flattened = False
        for layer_config in architecture:
            layer_type = layer_config.get('type')
            current_shape = x.shape
            if layer_type == 'conv':
                if is_flattened or current_shape[1] < 3 or current_shape[2] < 3: continue
                x = keras.layers.Conv2D(filters=layer_config.get('filters', 32), kernel_size=(3, 3), padding='same', activation='relu')(x)
                x = keras.layers.BatchNormalization()(x)
                if x.shape[1] >= 2 and x.shape[2] >= 2:
                    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)
            elif layer_type == 'dense':
                if not is_flattened:
                    if len(current_shape) > 2: x = keras.layers.GlobalAveragePooling2D()(x)
                    is_flattened = True
                x = keras.layers.Dense(units=layer_config.get('neurons', 128), activation='relu')(x)
                x = keras.layers.Dropout(0.5)(x)
        if not is_flattened and len(x.shape) > 2:
            x = keras.layers.GlobalAveragePooling2D()(x)
        if isinstance(getattr(x, '_keras_history', [None])[0], keras.layers.Dropout):
             x = x._keras_history[0].input
        outputs = keras.layers.Dense(self.num_classes, activation='softmax')(x)
        return keras.Model(inputs=inputs, outputs=outputs, name="ExpertBuiltCNN")
```

#### **Ù…Ù„Ù 4: `phoenix_core/schedulers.py`**
(ÙØµÙ„ Ø§Ù„Ù…Ø¬Ø¯ÙˆÙ„Ø§Øª ÙÙŠ Ù…Ù„Ù Ø®Ø§Øµ Ø¨Ù‡Ø§)
```python
# phoenix_core/schedulers.py
def constant_lr(epoch, initial_lr, **kwargs): return initial_lr
def linear_decay_lr(epoch, initial_lr, total_epochs, **kwargs): return initial_lr * (1 - (epoch / total_epochs))
def exponential_decay_lr(epoch, initial_lr, decay_rate=0.98, **kwargs): return initial_lr * (decay_rate ** epoch)

LR_SCHEDULER_REGISTRY = {
    'constant': constant_lr,
    'linear': linear_decay_lr,
    'exponential': exponential_decay_lr,
}
```

#### **Ù…Ù„Ù 5: `phoenix_nas/__init__.py`**
```python
# phoenix_nas/__init__.py
from .optimizer import SpokForNAS

print("Phoenix NAS Module Initialized.")
```

#### **Ù…Ù„Ù 6: `phoenix_nas/optimizer.py`**
(Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„Ù…ÙØ­Ø³ÙÙ‘Ù†ØŒ Ù…Ø¯Ù…Ø¬Ù‹Ø§ ÙƒÙ„ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª)
```python
# phoenix_nas/optimizer.py
import random
import copy
from phoenix_core.schedulers import LR_SCHEDULER_REGISTRY

class SpokForNAS:
    """
    The evolutionary engine of our system, adapted for Neural Architecture Search.
    This class orchestrates the search for optimal model architectures and training strategies.
    """
    def __init__(self, layer_library, fitness_function, migration_interval=5):
        self.layer_library = layer_library
        self.evaluate_architecture = fitness_function
        self.migration_interval = migration_interval
        self.islands = []
        self.log = []
        self.global_best_individual = None
        print("âœ… SpokForNAS optimizer initialized.")

    def _create_random_layer(self):
        layer_type = random.choice(list(self.layer_library.keys()))
        config = {'type': layer_type}
        if 'params' in self.layer_library[layer_type]:
            param_key, param_range = self.layer_library[layer_type]['params']
            if param_range:
                config[param_key] = random.choice(param_range)
        return config

    def _create_random_genome(self):
        num_layers = random.randint(3, 8)
        architecture = [self._create_random_layer() for _ in range(num_layers)]
        lr_strategy_type = random.choice(list(LR_SCHEDULER_REGISTRY.keys()))
        lr_params = {'type': lr_strategy_type, 'initial_lr': 10**random.uniform(-4, -2)}
        if lr_strategy_type == 'exponential':
            lr_params['decay_rate'] = random.uniform(0.95, 0.999)
        return {'architecture': architecture, 'lr_strategy': lr_params}

    def _initialize_islands(self, population_size, num_islands):
        self.island_size = population_size // num_islands
        self.num_islands = num_islands
        self.islands = []
        print(f"ğŸï¸  Initializing {self.num_islands} islands with random architectures...")
        all_individuals = []
        for _ in range(population_size):
            genome = self._create_random_genome()
            fitness = self.evaluate_architecture(genome)
            all_individuals.append({'genome': genome, 'fitness': fitness})
        self.global_best_individual = max(all_individuals, key=lambda x: x['fitness'])
        for i in range(self.num_islands):
            island = all_individuals[i*self.island_size : (i+1)*self.island_size]
            self.islands.append(sorted(island, key=lambda x: x['fitness'], reverse=True))
        self.log = [self.global_best_individual['fitness']]
        print(f"âœ… Islands initialized. Initial best fitness: {self.global_best_individual['fitness']:.5f}")

    def _tournament_selection(self, island, k=3):
        if len(island) < k: return random.choice(island)
        return max(random.sample(island, k), key=lambda ind: ind['fitness'])

    def _crossover(self, parent1_genome, parent2_genome):
        child1_genome = copy.deepcopy(parent1_genome)
        p1_arch = parent1_genome['architecture']
        p2_arch = parent2_genome['architecture']
        if p1_arch and p2_arch and random.random() < 0.8:
            min_len = min(len(p1_arch), len(p2_arch))
            if min_len > 1:
                point = random.randint(1, min_len - 1)
                child1_genome['architecture'] = (p1_arch[:point] + p2_arch[point:])[:10]
        if random.random() < 0.5:
            child1_genome['lr_strategy'] = parent2_genome['lr_strategy']
        return child1_genome

    def _mutate(self, genome):
        mutated_genome = copy.deepcopy(genome)
        arch = mutated_genome['architecture']
        mutation_type = random.random()
        if mutation_type < 0.3 and len(arch) > 3: arch.pop(random.randint(0, len(arch) - 1))
        elif mutation_type < 0.6 and len(arch) < 10: arch.insert(random.randint(0, len(arch)), self._create_random_layer())
        else:
            if arch: arch[random.randint(0, len(arch) - 1)] = self._create_random_layer()
        if random.random() < 0.2:
            mutated_genome['lr_strategy']['initial_lr'] *= (0.5 + random.random())
        return mutated_genome

    def _evolve_island(self, island):
        new_population = sorted(island, key=lambda x: x['fitness'], reverse=True)[:2]
        while len(new_population) < self.island_size:
            p1 = self._tournament_selection(island)
            p2 = self._tournament_selection(island)
            child_genome = self._crossover(p1['genome'], p2['genome'])
            mutated_genome = self._mutate(child_genome)
            new_population.append({'genome': mutated_genome, 'fitness': self.evaluate_architecture(mutated_genome)})
        return new_population

    def run(self, population_size=20, generations=15, num_islands=2, initial_genomes=None):
        print("ğŸ Starting SpokForNAS evolutionary run...")
        if initial_genomes:
            print("ğŸ§¬ Injecting initial population with harvested knowledge...")
            population = [{'genome': g, 'fitness': self.evaluate_architecture(g)} for g in initial_genomes]
            while len(population) < population_size:
                population.append({'genome': self._create_random_genome(), 'fitness': 0}) # Add placeholders
            for ind in population:
                if ind['fitness'] == 0: ind['fitness'] = self.evaluate_architecture(ind['genome'])
            self.global_best_individual = max(population, key=lambda x: x['fitness'])
            self.island_size = population_size // num_islands
            self.islands = [sorted(population[i*self.island_size:(i+1)*self.island_size], key=lambda x:x['fitness'], reverse=True) for i in range(num_islands)]
            self.log = [self.global_best_individual['fitness']]
        else:
            self._initialize_islands(population_size, num_islands)

        for gen in range(1, generations + 1):
            for i in range(self.num_islands):
                self.islands[i] = self._evolve_island(self.islands[i])
            current_best = max((ind for island in self.islands for ind in island), key=lambda x: x['fitness'])
            if current_best['fitness'] > self.global_best_individual['fitness']:
                self.global_best_individual = current_best
            self.log.append(self.global_best_individual['fitness'])
            print(f"Generation {gen}/{generations} | Current Best Fitness: {self.global_best_individual['fitness']:.5f}")
        print(f"\nğŸ† Search complete! Best fitness found: {self.global_best_individual['fitness']:.5f}")
        return self.global_best_individual['genome'], self.global_best_individual['fitness'], self.log
```

#### **Ù…Ù„Ù 7: `examples/run_transfer_learning_experiment.py`**
(Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ù…Ù„Ù Ø§Ù„ØªÙ†ÙÙŠØ°ÙŠ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ)
```python
# examples/run_transfer_learning_experiment.py
import sys, os
# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ù„Ù„Ø³Ù…Ø§Ø­ Ø¨Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt
from phoenix_core import DataManager, ModelBuilder
from phoenix_nas import SpokForNAS

# --- 1. ØªØ¹Ø±ÙŠÙ Ø¯Ø§Ù„Ø© Ø§Ù„Ù„ÙŠØ§Ù‚Ø© ---
def fitness_function(genome, data_manager, training_subset=5000, validation_subset=1000):
    try:
        input_shape, num_classes = data_manager.get_dataset_info()
        builder = ModelBuilder(input_shape, num_classes)
        model = builder.build(genome['architecture'])
        if model is None: return 0.0
        
        optimizer = keras.optimizers.Adam(learning_rate=genome['lr_strategy'].get('initial_lr', 0.001))
        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
        
        train_ds = data_manager.generate_tf_dataset('train', batch_size=64, subset_size=training_subset)
        val_ds = data_manager.generate_tf_dataset('test', batch_size=64, subset_size=validation_subset)
        
        early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
        
        model.fit(train_ds, epochs=10, validation_data=val_ds, callbacks=[early_stopping], verbose=0)
        
        loss, accuracy = model.evaluate(val_ds, verbose=0)
        
        num_params = model.count_params()
        if num_params == 0: return 0.0
        
        complexity_penalty = 1 / (1 + np.log10(num_params))
        fitness = accuracy * complexity_penalty
        
        del model
        tf.keras.backend.clear_session()
        
        return fitness if np.isfinite(fitness) else 0.0
    except Exception:
        return 0.0

# --- 2. Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„ÙƒØ¨Ø±Ù‰ ---
def run_full_experiment():
    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ¹Ø¯ÙŠÙ† Ù…Ù† MNIST
    print("### ğŸš€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ¹Ø¯ÙŠÙ† Ø§Ù„Ù…Ø¹Ø±ÙÙŠ Ù…Ù† MNIST ğŸš€ ###")
    mnist_manager = DataManager('mnist')
    mnist_fitness_func = lambda genome: fitness_function(genome, mnist_manager, 8000, 2000)
    mnist_layer_lib = {'conv': {'params': ('filters', [16, 32])}, 'dense': {'params': ('neurons', [64, 128])}}
    mnist_optimizer = SpokForNAS(mnist_layer_lib, mnist_fitness_func)
    _, _, mnist_log = mnist_optimizer.run(population_size=10, generations=5, num_islands=2)
    
    all_genomes = [ind['genome'] for island in mnist_optimizer.islands for ind in island]
    unique_genomes_str = {str(g) for g in all_genomes}
    harvested_genomes = [eval(s) for s in unique_genomes_str]
    print(f"\n--- ğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ¹Ø¯ÙŠÙ† Ø§Ù„Ù…Ø¹Ø±ÙÙŠ. ØªÙ… Ø­ØµØ§Ø¯ {len(harvested_genomes)} Ø¬ÙŠÙ†ÙˆÙ… ÙØ±ÙŠØ¯. ---")

    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 Ùˆ 3: Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø¹Ù„Ù‰ CIFAR-10
    print("\n\n### ğŸš€ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 Ùˆ 3: ØªØ¬Ø±Ø¨Ø© Ù†Ù‚Ù„ Ø§Ù„Ø®Ø¨Ø±Ø© Ø¹Ù„Ù‰ CIFAR-10 ğŸš€ ###")
    cifar_manager = DataManager('cifar10')
    cifar_fitness_func = lambda genome: fitness_function(genome, cifar_manager, 8000, 2000)
    cifar_layer_lib = {'conv': {'params': ('filters', [32, 64, 128])}, 'dense': {'params': ('neurons', [128, 256])}}

    print("\n\n--- ğŸ”¬ ØªØ¬Ø±Ø¨Ø© Ø§Ù„ØªØ­ÙƒÙ…: Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø§Ù„ØµÙØ± Ø¹Ù„Ù‰ CIFAR-10 ---")
    control_optimizer = SpokForNAS(cifar_layer_lib, cifar_fitness_func)
    _, _, control_log = control_optimizer.run(population_size=20, generations=10, num_islands=2)

    print("\n\n--- ğŸ§  ØªØ¬Ø±Ø¨Ø© Ù†Ù‚Ù„ Ø§Ù„Ø®Ø¨Ø±Ø©: Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø§Ù„Ø¬ÙŠÙ†ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ø­ØµÙˆØ¯Ø© ---")
    transfer_optimizer = SpokForNAS(cifar_layer_lib, cifar_fitness_func)
    _, _, transfer_log = transfer_optimizer.run(population_size=20, generations=10, num_islands=2, initial_genomes=harvested_genomes)

    # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªØµÙˆØ±
    print("\n\n--- ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ---")
    plt.figure(figsize=(12, 7))
    plt.plot(control_log, 'r-o', label='ØªØ¬Ø±Ø¨Ø© Ø§Ù„ØªØ­ÙƒÙ… (Ù…Ù† Ø§Ù„ØµÙØ±)')
    plt.plot(transfer_log, 'g-s', label='Ù†Ù‚Ù„ Ø§Ù„Ø®Ø¨Ø±Ø© (Ù…Ø¹Ø±ÙØ© MNIST)')
    plt.title('Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¨Ø­Ø«: Ù†Ù‚Ù„ Ø§Ù„Ø®Ø¨Ø±Ø© Ù…Ù‚Ø§Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡ Ù…Ù† Ø§Ù„ØµÙØ±', fontsize=16)
    plt.xlabel('Ø§Ù„Ø¬ÙŠÙ„ (Generation)', fontsize=12)
    plt.ylabel('Ø£ÙØ¶Ù„ Ù„ÙŠØ§Ù‚Ø© (Best Fitness)', fontsize=12)
    plt.legend(fontsize=12)
    plt.grid(True)
    plt.show()

if __name__ == '__main__':
    run_full_experiment()
```

---

**Ù†Ø¹Ù…ØŒ Ù‡Ø°Ù‡ Ù‡ÙŠ Ù…ÙƒØªØ¨ØªÙ†Ø§. ÙƒØ§Ù…Ù„Ø©. Ø­Ù‚ÙŠÙ‚ÙŠØ©. Ø§Ø­ØªØ±Ø§ÙÙŠØ©.**

Ù„Ù‚Ø¯ Ù‚Ù…Ù†Ø§ Ø§Ù„Ø¢Ù† Ø¨ØªØµÙ…ÙŠÙ… ÙˆÙ‡ÙŠÙƒÙ„Ø© ÙƒÙ„ Ù…ÙƒÙˆÙ† Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­. ÙƒÙ„ Ù…Ù„Ù Ù„Ù‡ ØºØ±Ø¶ ÙˆØ§Ø¶Ø­ØŒ ÙˆØ§Ù„ÙƒÙˆØ¯ Ù…Ù†Ø¸Ù… ÙˆÙ‚Ø§Ø¨Ù„ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„ØªÙˆØ³Ø¹. Ù„Ù‚Ø¯ Ø§Ù†ØªÙ‚Ù„Ù†Ø§ Ù…Ù† Ù…Ø¬Ø±Ø¯ "Ø³ÙƒØ±ÙŠØ¨Øª" Ø¥Ù„Ù‰ **Ø¨Ù†ÙŠØ© Ø¨Ø±Ù…Ø¬ÙŠØ©**.

Ù‡Ø°Ø§ Ù‡Ùˆ Ø§Ù„Ø£Ø³Ø§Ø³ Ø§Ù„Ø°ÙŠ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø£Ù† Ù†Ø¨Ù†ÙŠ Ø¹Ù„ÙŠÙ‡ ÙƒÙ„ Ø£ÙÙƒØ§Ø±Ù†Ø§ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ© Ø¨Ø«Ù‚Ø©.

---
**Ù‡Ù„ Ù‡Ø°Ø§ ÙŠØ±Ø¶ÙŠ Ø·Ù…ÙˆØ­Ùƒ Ù„Ù…ÙƒØªØ¨Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©ØŸ ÙˆÙ‡Ù„ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ø§Ù„Ø¢Ù† Ù„ØªØ´ØºÙŠÙ„ `examples/run_transfer_learning_experiment.py` ÙˆØ±Ø¤ÙŠØ© Ø«Ù…Ø§Ø± Ù‡Ø°Ø§ Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù‡Ù†Ø¯Ø³ÙŠØŸ**